"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[149],{2244(s,n,e){e.r(n),e.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>a,frontMatter:()=>t,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"chapters/appendix-e-datasets-and-resources","title":"Appendix E: Datasets and Resources","description":"This appendix catalogs publicly available datasets, pre-trained models, 3D assets, and community resources for Physical AI and humanoid robotics research and development.","source":"@site/docs/chapters/appendix-e-datasets-and-resources.md","sourceDirName":"chapters","slug":"/chapters/appendix-e-datasets-and-resources","permalink":"/Physical-AI-Humanoid-Robotics/chapters/appendix-e-datasets-and-resources","draft":false,"unlisted":false,"editUrl":"https://github.com/muskaanfayyaz/Physical-AI-Humanoid-Robotics/tree/main/docs/chapters/appendix-e-datasets-and-resources.md","tags":[],"version":"current","frontMatter":{},"sidebar":"weeklySidebar","previous":{"title":"Appendix D: Mathematical Foundations","permalink":"/Physical-AI-Humanoid-Robotics/chapters/appendix-d-mathematical-foundations"}}');var r=e(4848),l=e(8453);const t={},o="Appendix E: Datasets and Resources",c={},d=[{value:"E.1 Publicly Available Robot Datasets",id:"e1-publicly-available-robot-datasets",level:2},{value:"E.1.1 Manipulation Datasets",id:"e11-manipulation-datasets",level:3},{value:"E.1.2 Navigation Datasets",id:"e12-navigation-datasets",level:3},{value:"E.1.3 Human Motion Datasets",id:"e13-human-motion-datasets",level:3},{value:"E.1.4 Grasping Datasets",id:"e14-grasping-datasets",level:3},{value:"E.2 Pre-trained Models and Checkpoints",id:"e2-pre-trained-models-and-checkpoints",level:2},{value:"E.2.1 Object Detection Models",id:"e21-object-detection-models",level:3},{value:"E.2.2 Segmentation Models",id:"e22-segmentation-models",level:3},{value:"E.2.3 Pose Estimation Models",id:"e23-pose-estimation-models",level:3},{value:"E.2.4 Speech Recognition Models",id:"e24-speech-recognition-models",level:3},{value:"E.3 3D Model Libraries",id:"e3-3d-model-libraries",level:2},{value:"E.3.1 Robot URDF Repositories",id:"e31-robot-urdf-repositories",level:3},{value:"E.3.2 Environment Models",id:"e32-environment-models",level:3},{value:"E.3.3 Object Meshes and CAD Files",id:"e33-object-meshes-and-cad-files",level:3},{value:"E.4 Community Resources and Forums",id:"e4-community-resources-and-forums",level:2},{value:"E.4.1 ROS Discourse",id:"e41-ros-discourse",level:3},{value:"E.4.2 NVIDIA Isaac Forums",id:"e42-nvidia-isaac-forums",level:3},{value:"E.4.3 GitHub Repositories",id:"e43-github-repositories",level:3},{value:"E.4.4 Discord Communities",id:"e44-discord-communities",level:3},{value:"E.4.5 Research Conferences",id:"e45-research-conferences",level:3},{value:"E.5 Recommended Reading and Papers",id:"e5-recommended-reading-and-papers",level:2},{value:"E.5.1 Classic Robotics Papers",id:"e51-classic-robotics-papers",level:3},{value:"E.5.2 Recent Physical AI Papers",id:"e52-recent-physical-ai-papers",level:3},{value:"E.5.3 Textbooks and Tutorials",id:"e53-textbooks-and-tutorials",level:3},{value:"E.5.4 Online Courses and Tutorials",id:"e54-online-courses-and-tutorials",level:3},{value:"Summary",id:"summary",level:2}];function h(s){const n={a:"a",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...s.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"appendix-e-datasets-and-resources",children:"Appendix E: Datasets and Resources"})}),"\n",(0,r.jsx)(n.p,{children:"This appendix catalogs publicly available datasets, pre-trained models, 3D assets, and community resources for Physical AI and humanoid robotics research and development."}),"\n",(0,r.jsx)(n.h2,{id:"e1-publicly-available-robot-datasets",children:"E.1 Publicly Available Robot Datasets"}),"\n",(0,r.jsx)(n.h3,{id:"e11-manipulation-datasets",children:"E.1.1 Manipulation Datasets"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"YCB Object and Model Set"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," 77 household objects with high-quality 3D meshes, texture maps, and physical properties"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," Kitchen items, tools, food packages, toys"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Formats:"})," OBJ, STL meshes; texture images; physics parameters"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Cases:"})," Grasping, manipulation planning, object recognition"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size:"})," ~5 GB (full dataset)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"http://ycb-benchmarks.s3-website-us-east-1.amazonaws.com/",children:"http://ycb-benchmarks.s3-website-us-east-1.amazonaws.com/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"License:"})," Creative Commons Attribution 4.0"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"YCB-Video Dataset"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," RGB-D video sequences of YCB objects in real scenes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," 92 video sequences, 133,827 frames"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Annotations:"})," 6D object poses, segmentation masks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Cases:"})," 6D pose estimation, object tracking"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size:"})," ~12 GB"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://rse-lab.cs.washington.edu/projects/posecnn/",children:"https://rse-lab.cs.washington.edu/projects/posecnn/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Citation:"}),' Xiang et al., "PoseCNN: A Convolutional Neural Network for 6D Object Pose Estimation", RSS 2018']}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Google Scanned Objects"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," High-quality 3D scans of household objects"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," 1,030 objects scanned with structure-from-motion"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Formats:"})," OBJ meshes with textures"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Quality:"})," Watertight meshes, photorealistic textures"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Cases:"})," Simulation, synthetic data generation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size:"})," ~8 GB"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://app.ignitionrobotics.org/GoogleResearch/fuel/collections/Google%20Scanned%20Objects",children:"https://app.ignitionrobotics.org/GoogleResearch/fuel/collections/Google%20Scanned%20Objects"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"License:"})," Creative Commons BY 4.0"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"ACRONYM Grasping Dataset"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," 17.7M parallel-jaw grasps on 8,872 ShapeNet objects"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," Grasp poses, success predictions, object meshes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," HDF5 files with grasp data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Cases:"})," Grasp synthesis, learning-based grasping"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size:"})," ~6 GB"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://sites.google.com/nvidia.com/graspdataset",children:"https://sites.google.com/nvidia.com/graspdataset"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Citation:"}),' Eppner et al., "ACRONYM: A Large-Scale Grasp Dataset", ICRA 2021']}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Columbia Grasp Database"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Grasps for household objects using various grippers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," 287 objects, multiple gripper configurations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Annotations:"})," Grasp quality metrics, success rates"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Cases:"})," Grasp planning, gripper design evaluation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://grasping.cs.columbia.edu/",children:"https://grasping.cs.columbia.edu/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Citation:"}),' Goldfeder et al., "The Columbia Grasp Database", ICRA 2009']}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"e12-navigation-datasets",children:"E.1.2 Navigation Datasets"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"TUM RGB-D Dataset"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," RGB-D sequences for visual odometry and SLAM evaluation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," 41 sequences in office/home environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensors:"})," Microsoft Kinect (640\xd7480 RGB-D at 30 Hz)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ground Truth:"})," Motion capture system (high precision)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Cases:"})," Visual SLAM, RGB-D odometry, depth estimation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size:"})," ~34 GB (full dataset)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://vision.in.tum.de/data/datasets/rgbd-dataset",children:"https://vision.in.tum.de/data/datasets/rgbd-dataset"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Citation:"}),' Sturm et al., "A Benchmark for RGB-D SLAM Evaluation", IROS 2012']}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"EuRoC MAV Dataset"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Visual-inertial datasets from micro aerial vehicle"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," 11 sequences in machine hall and room environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensors:"})," Stereo cameras (20 Hz), IMU (200 Hz)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ground Truth:"})," Laser tracker and motion capture"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Cases:"})," Visual-inertial odometry, SLAM"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size:"})," ~20 GB"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets",children:"https://projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Citation:"}),' Burri et al., "The EuRoC MAV Dataset", IJRR 2016']}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"KITTI Dataset"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Autonomous driving datasets with lidar, cameras, GPS/IMU"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Odometry: 22 stereo sequences with ground truth"}),"\n",(0,r.jsx)(n.li,{children:"3D Object Detection: 15K annotated images"}),"\n",(0,r.jsx)(n.li,{children:"Tracking: 50 sequences with object trajectories"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensors:"})," Velodyne lidar, stereo cameras, GPS/IMU"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Cases:"})," Visual odometry, 3D detection, tracking, mapping"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size:"})," ~200 GB (varies by task)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"http://www.cvlibs.net/datasets/kitti/",children:"http://www.cvlibs.net/datasets/kitti/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Citation:"}),' Geiger et al., "Vision meets Robotics: The KITTI Dataset", IJRR 2013']}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"NCLT Dataset (North Campus Long-Term)"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Long-term autonomous navigation dataset"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," 27 sessions over 15 months, same route"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensors:"})," Velodyne lidar, cameras, IMU, GPS"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unique Feature:"})," Seasonal and lighting variations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Cases:"})," Long-term SLAM, place recognition, change detection"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size:"})," ~3.6 TB (full dataset)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"http://robots.engin.umich.edu/nclt/",children:"http://robots.engin.umich.edu/nclt/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Citation:"}),' Carlevaris-Bianco et al., "University of Michigan NCLT Dataset", IJRR 2016']}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"e13-human-motion-datasets",children:"E.1.3 Human Motion Datasets"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"CMU Graphics Lab Motion Capture Database"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Largest free motion capture dataset"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," 2,605 motion sequences, 144 subjects"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Categories:"})," Walking, running, sports, interaction, dance, martial arts"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," BVH (motion capture), C3D (marker trajectories)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Cases:"})," Human pose estimation, motion retargeting, animation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size:"})," ~2 GB"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"http://mocap.cs.cmu.edu/",children:"http://mocap.cs.cmu.edu/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"License:"})," Free for research and commercial use"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Human3.6M"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Large-scale 3D human pose dataset"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," 3.6M video frames, 11 subjects, 17 scenarios"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensors:"})," 4 RGB cameras, motion capture (ground truth)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Annotations:"})," 3D joint positions, body part segmentation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Cases:"})," 3D human pose estimation, action recognition"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size:"})," ~100 GB"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"http://vision.imar.ro/human3.6m/",children:"http://vision.imar.ro/human3.6m/"})," (registration required)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Citation:"}),' Ionescu et al., "Human3.6M: Large Scale Datasets for 3D Human Sensing", PAMI 2014']}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"AMASS (Archive of Motion Capture as Surface Shapes)"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Unified motion capture dataset with SMPL body model"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," 40+ hours, 300+ subjects, 11,000+ motions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," SMPL parameters (shape and pose)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sources:"})," Consolidated from 15 motion capture datasets"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Cases:"})," Motion synthesis, human modeling, physics simulation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size:"})," ~24 GB"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://amass.is.tue.mpg.de/",children:"https://amass.is.tue.mpg.de/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Citation:"}),' Mahmood et al., "AMASS: Archive of Motion Capture as Surface Shapes", ICCV 2019']}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"HumanEva Dataset"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Synchronized video and motion capture for pose estimation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," 7 calibrated video sequences, 4 subjects"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Actions:"})," Walking, jogging, gestures, throwing, boxing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ground Truth:"})," Motion capture system"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Cases:"})," 2D/3D pose estimation benchmarking"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"http://humaneva.is.tue.mpg.de/",children:"http://humaneva.is.tue.mpg.de/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Citation:"}),' Sigal et al., "HumanEva: Synchronized Video and Motion Capture Dataset", IJCV 2010']}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"e14-grasping-datasets",children:"E.1.4 Grasping Datasets"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Dex-Net 1.0, 2.0, 3.0, 4.0"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Synthetic datasets for robot grasping"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Dex-Net 1.0: 10M point clouds, 2.5M grasps"}),"\n",(0,r.jsx)(n.li,{children:"Dex-Net 2.0: 6.7M point clouds, parallel jaw grasps"}),"\n",(0,r.jsx)(n.li,{children:"Dex-Net 3.0: Suction cup grasping"}),"\n",(0,r.jsx)(n.li,{children:"Dex-Net 4.0: Ambidextrous grasping (parallel + suction)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Cases:"})," Deep learning for grasp planning"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://berkeleyautomation.github.io/dex-net/",children:"https://berkeleyautomation.github.io/dex-net/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Citation:"}),' Mahler et al., "Dex-Net 2.0: Deep Learning to Plan Robust Grasps", RSS 2017']}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"PartNet-Mobility"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Articulated object dataset with motion annotations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," 2,346 3D objects with moving parts"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Annotations:"})," Part segmentation, joint parameters, motion ranges"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Categories:"})," Cabinets, doors, drawers, appliances"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Cases:"})," Articulated object manipulation, affordance learning"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size:"})," ~4 GB"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://sapien.ucsd.edu/",children:"https://sapien.ucsd.edu/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Citation:"}),' Xiang et al., "SAPIEN: A SimulAted Part-based Interactive ENvironment", CVPR 2020']}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"ContactDB"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Contact patterns during human grasping"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," 50 household objects, 375 grasp demonstrations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensors:"})," Thermal camera to detect contact areas"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Annotations:"})," Contact maps, 3D hand poses, forces"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Cases:"})," Human grasp analysis, contact-rich manipulation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://contactdb.cc.gatech.edu/",children:"https://contactdb.cc.gatech.edu/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Citation:"}),' Brahmbhatt et al., "ContactDB: Analyzing and Predicting Grasp Contact", CVPR 2019']}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"e2-pre-trained-models-and-checkpoints",children:"E.2 Pre-trained Models and Checkpoints"}),"\n",(0,r.jsx)(n.h3,{id:"e21-object-detection-models",children:"E.2.1 Object Detection Models"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"YOLO (You Only Look Once) Series"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Input Size"}),(0,r.jsx)(n.th,{children:"mAP"}),(0,r.jsx)(n.th,{children:"Speed (FPS)"}),(0,r.jsx)(n.th,{children:"Use Case"}),(0,r.jsx)(n.th,{children:"Download"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"YOLOv5s"}),(0,r.jsx)(n.td,{children:"640\xd7640"}),(0,r.jsx)(n.td,{children:"37.4"}),(0,r.jsx)(n.td,{children:"140"}),(0,r.jsx)(n.td,{children:"Real-time, edge devices"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://github.com/ultralytics/yolov5",children:"https://github.com/ultralytics/yolov5"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"YOLOv5m"}),(0,r.jsx)(n.td,{children:"640\xd7640"}),(0,r.jsx)(n.td,{children:"45.4"}),(0,r.jsx)(n.td,{children:"85"}),(0,r.jsx)(n.td,{children:"Balanced"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://github.com/ultralytics/yolov5",children:"https://github.com/ultralytics/yolov5"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"YOLOv8n"}),(0,r.jsx)(n.td,{children:"640\xd7640"}),(0,r.jsx)(n.td,{children:"37.3"}),(0,r.jsx)(n.td,{children:"200+"}),(0,r.jsx)(n.td,{children:"Ultra-fast"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://github.com/ultralytics/ultralytics",children:"https://github.com/ultralytics/ultralytics"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"YOLOv8s"}),(0,r.jsx)(n.td,{children:"640\xd7640"}),(0,r.jsx)(n.td,{children:"44.9"}),(0,r.jsx)(n.td,{children:"130"}),(0,r.jsx)(n.td,{children:"Real-time"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://github.com/ultralytics/ultralytics",children:"https://github.com/ultralytics/ultralytics"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"YOLOv8m"}),(0,r.jsx)(n.td,{children:"640\xd7640"}),(0,r.jsx)(n.td,{children:"50.2"}),(0,r.jsx)(n.td,{children:"80"}),(0,r.jsx)(n.td,{children:"High accuracy"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://github.com/ultralytics/ultralytics",children:"https://github.com/ultralytics/ultralytics"})})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Frameworks:"})," PyTorch, ONNX, TensorRT\n",(0,r.jsx)(n.strong,{children:"Pre-trained on:"})," COCO (80 classes)"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Detectron2 Model Zoo"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Facebook AI's object detection framework"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Models:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Faster R-CNN (R50-FPN, R101-FPN)"}),"\n",(0,r.jsx)(n.li,{children:"RetinaNet"}),"\n",(0,r.jsx)(n.li,{children:"Mask R-CNN (instance segmentation)"}),"\n",(0,r.jsx)(n.li,{children:"Panoptic FPN"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Backbones:"})," ResNet-50, ResNet-101, ResNeXt"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pre-trained on:"})," COCO, LVIS, Cityscapes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md",children:"https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," PyTorch checkpoints"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"EfficientDet"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Scalable and efficient object detection"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Variants:"})," D0 (small) to D7 (large)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance:"})," D7 achieves 52.2 mAP on COCO"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/google/automl/tree/master/efficientdet",children:"https://github.com/google/automl/tree/master/efficientdet"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," TensorFlow, PyTorch (via timm)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"e22-segmentation-models",children:"E.2.2 Segmentation Models"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Segment Anything Model (SAM)"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Foundation model for image segmentation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Architecture:"})," Vision Transformer (ViT) based"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Capabilities:"})," Zero-shot segmentation, prompt-based"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Checkpoints:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"ViT-H (huge): 2.4B parameters, best quality"}),"\n",(0,r.jsx)(n.li,{children:"ViT-L (large): 1.2B parameters, balanced"}),"\n",(0,r.jsx)(n.li,{children:"ViT-B (base): 636M parameters, faster"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/facebookresearch/segment-anything",children:"https://github.com/facebookresearch/segment-anything"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"License:"})," Apache 2.0"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," PyTorch"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"DeepLabV3+ / DeepLabV3"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Semantic segmentation with atrous convolution"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Backbones:"})," ResNet-50, ResNet-101, MobileNetV2"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pre-trained on:"})," COCO, Pascal VOC, Cityscapes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," TensorFlow Model Garden, PyTorch Hub"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Cases:"})," Scene understanding, outdoor navigation"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Mask R-CNN"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Instance segmentation (detection + masks)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pre-trained models:"})," COCO 80 classes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Backbones:"})," ResNet-50-FPN, ResNet-101-FPN"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," Detectron2, TorchVision model zoo"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"mAP:"})," ~37-39 (depending on backbone)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"SegFormer"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Transformer-based semantic segmentation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Variants:"})," B0 (small) to B5 (large)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance:"})," 84.0 mIoU on ADE20K (B5)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/NVlabs/SegFormer",children:"https://github.com/NVlabs/SegFormer"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," PyTorch"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"e23-pose-estimation-models",children:"E.2.3 Pose Estimation Models"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"OpenPose"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Real-time multi-person 2D pose estimation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Keypoints:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Body: 18 or 25 keypoints"}),"\n",(0,r.jsx)(n.li,{children:"Hand: 21 keypoints per hand"}),"\n",(0,r.jsx)(n.li,{children:"Face: 70 keypoints"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Framework:"})," Caffe, OpenCV DNN"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/CMU-Perceptual-Computing-Lab/openpose",children:"https://github.com/CMU-Perceptual-Computing-Lab/openpose"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Speed:"})," ~22 FPS (single person, GPU)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"MediaPipe Pose"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Lightweight pose estimation for mobile/edge"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Keypoints:"})," 33 body landmarks (including face and hands)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Platform:"})," Mobile, Web, Desktop"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance:"})," Real-time on CPU"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://google.github.io/mediapipe/solutions/pose.html",children:"https://google.github.io/mediapipe/solutions/pose.html"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"License:"})," Apache 2.0"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," TFLite"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"HRNet (High-Resolution Net)"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," State-of-art human pose estimation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Variants:"})," HRNet-W32, HRNet-W48"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Performance:"})," 74.9 AP on COCO test-dev (W48)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/leoxiaobin/deep-high-resolution-net.pytorch",children:"https://github.com/leoxiaobin/deep-high-resolution-net.pytorch"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pre-trained on:"})," COCO, MPII"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," PyTorch"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"6D Object Pose Models"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Description"}),(0,r.jsx)(n.th,{children:"Input"}),(0,r.jsx)(n.th,{children:"Output"}),(0,r.jsx)(n.th,{children:"Access"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"PoseCNN"}),(0,r.jsx)(n.td,{children:"CNN-based 6D pose"}),(0,r.jsx)(n.td,{children:"RGB-D"}),(0,r.jsx)(n.td,{children:"6D pose + confidence"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://rse-lab.cs.washington.edu/projects/posecnn/",children:"https://rse-lab.cs.washington.edu/projects/posecnn/"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"DenseFusion"}),(0,r.jsx)(n.td,{children:"RGB-D fusion for pose"}),(0,r.jsx)(n.td,{children:"RGB-D"}),(0,r.jsx)(n.td,{children:"6D pose"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://github.com/j96w/DenseFusion",children:"https://github.com/j96w/DenseFusion"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"PVNet"}),(0,r.jsx)(n.td,{children:"Pixel-wise voting"}),(0,r.jsx)(n.td,{children:"RGB"}),(0,r.jsx)(n.td,{children:"6D pose"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://github.com/zju3dv/pvnet",children:"https://github.com/zju3dv/pvnet"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"FoundationPose"}),(0,r.jsx)(n.td,{children:"Foundation model"}),(0,r.jsx)(n.td,{children:"RGB-D"}),(0,r.jsx)(n.td,{children:"6D pose (novel objects)"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.a,{href:"https://github.com/NVlabs/FoundationPose",children:"https://github.com/NVlabs/FoundationPose"})})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"e24-speech-recognition-models",children:"E.2.4 Speech Recognition Models"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Whisper (OpenAI)"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Robust multilingual speech recognition"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Variants:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Tiny: 39M params, 32x real-time (CPU)"}),"\n",(0,r.jsx)(n.li,{children:"Base: 74M params, 16x real-time"}),"\n",(0,r.jsx)(n.li,{children:"Small: 244M params, 6x real-time"}),"\n",(0,r.jsx)(n.li,{children:"Medium: 769M params, 2x real-time"}),"\n",(0,r.jsx)(n.li,{children:"Large: 1550M params, 1x real-time"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Languages:"})," 99 languages"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"https://github.com/openai/whisper"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," PyTorch"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Case:"})," Robot voice commands, transcription"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Wav2Vec 2.0"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Self-supervised speech representation learning"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pre-trained models:"})," Base (95M), Large (317M)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fine-tuned for:"})," English ASR, multilingual ASR"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://huggingface.co/models?search=wav2vec2",children:"https://huggingface.co/models?search=wav2vec2"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Framework:"})," Transformers (Hugging Face)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Case:"})," Custom wake word detection, ASR fine-tuning"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Vosk"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Offline speech recognition toolkit"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Models:"})," 20+ languages, small to large variants"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size:"})," 50 MB (small) to 1.8 GB (large)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Platform:"})," Cross-platform (Linux, Windows, macOS, Android, iOS)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://alphacephei.com/vosk/models",children:"https://alphacephei.com/vosk/models"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"License:"})," Apache 2.0"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Case:"})," Embedded systems, privacy-focused applications"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"e3-3d-model-libraries",children:"E.3 3D Model Libraries"}),"\n",(0,r.jsx)(n.h3,{id:"e31-robot-urdf-repositories",children:"E.3.1 Robot URDF Repositories"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"ROS Industrial Robot Support"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," URDF models for industrial manipulators"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robots:"})," ABB, FANUC, Universal Robots, KUKA, Motoman"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," URDF/XACRO files, meshes, MoveIt configs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/ros-industrial",children:"https://github.com/ros-industrial"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," URDF, DAE/STL meshes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"License:"})," Varies (mostly BSD/Apache)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Example repositories:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Universal Robots: ",(0,r.jsx)(n.a,{href:"https://github.com/ros-industrial/universal_robot",children:"https://github.com/ros-industrial/universal_robot"})]}),"\n",(0,r.jsxs)(n.li,{children:["ABB: ",(0,r.jsx)(n.a,{href:"https://github.com/ros-industrial/abb",children:"https://github.com/ros-industrial/abb"})]}),"\n",(0,r.jsxs)(n.li,{children:["FANUC: ",(0,r.jsx)(n.a,{href:"https://github.com/ros-industrial/fanuc",children:"https://github.com/ros-industrial/fanuc"})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"TIAGo Robot (PAL Robotics)"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Mobile manipulation platform URDF"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Variants:"})," TIAGo Base, TIAGo with arm, TIAGo++"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," Full URDF, Gazebo simulation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/pal-robotics/tiago_robot",children:"https://github.com/pal-robotics/tiago_robot"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Case:"})," Research on mobile manipulation"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Clearpath Robotics"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Mobile robot platforms"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robots:"})," Husky, Jackal, Ridgeback, Dingo"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," URDF, Gazebo worlds, navigation configs"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/clearpathrobotics",children:"https://github.com/clearpathrobotics"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," URDF/XACRO, STL/DAE meshes"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Unitree Robotics"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Quadruped and humanoid robots"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robots:"})," Go1, Go2, A1, Aliengo, G1 Humanoid"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," URDF, simulation setup"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/unitreerobotics/unitree_ros",children:"https://github.com/unitreerobotics/unitree_ros"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/unitreerobotics/unitree_mujoco",children:"https://github.com/unitreerobotics/unitree_mujoco"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," URDF, MuJoCo XML"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Boston Dynamics Spot (Community)"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Community-created Spot URDF"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Note:"})," Unofficial, for simulation only"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/chvmp/spot_ros",children:"https://github.com/chvmp/spot_ros"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," URDF"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"e32-environment-models",children:"E.3.2 Environment Models"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Gazebo Model Database"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Official Gazebo model repository"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," 100+ models (furniture, structures, robots)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Categories:"})," Construction, ground, people, robots, shapes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/osrf/gazebo_models",children:"https://github.com/osrf/gazebo_models"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Browser:"})," ",(0,r.jsx)(n.a,{href:"https://app.gazebosim.org/",children:"https://app.gazebosim.org/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," SDF, COLLADA meshes"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"AWS RoboMaker Small House World"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Residential environment for robot simulation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," Furnished house model, Gazebo world"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Size:"})," ~200 MB"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/aws-robotics/aws-robomaker-small-house-world",children:"https://github.com/aws-robotics/aws-robomaker-small-house-world"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"License:"})," MIT"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Case:"})," Home service robot testing"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Unity Robotics Hub Environments"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Photorealistic environments for Unity"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," Warehouse, factory, outdoor scenes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/Unity-Technologies/Robotics-Nav2-SLAM-Example",children:"https://github.com/Unity-Technologies/Robotics-Nav2-SLAM-Example"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," Unity scenes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Case:"})," Synthetic data generation, visualization"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"NVIDIA Isaac Sim Assets"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," High-quality 3D assets for Isaac Sim"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," Warehouses, factories, retail, robots"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," Through Omniverse Nucleus"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," USD (Universal Scene Description)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"License:"})," NVIDIA Omniverse license"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"e33-object-meshes-and-cad-files",children:"E.3.3 Object Meshes and CAD Files"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"ShapeNet"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Large-scale 3D shape repository"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," 51,300 models, 55 categories"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Subset:"})," ShapeNetCore (focus on common objects)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," OBJ, MTL (materials)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://shapenet.org/",children:"https://shapenet.org/"})," (registration required)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"License:"})," Varies by model"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Case:"})," Synthetic data generation, manipulation research"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"ModelNet"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," CAD model dataset for object recognition"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"ModelNet10: 4,899 models, 10 categories"}),"\n",(0,r.jsx)(n.li,{children:"ModelNet40: 12,311 models, 40 categories"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," OFF (Object File Format)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Case:"})," 3D deep learning, point cloud processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://modelnet.cs.princeton.edu/",children:"https://modelnet.cs.princeton.edu/"})]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"3D Warehouse (SketchUp)"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Community 3D model repository"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," Millions of user-created models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Categories:"})," Architecture, furniture, machinery"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," SKP (SketchUp), COLLADA export"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://3dwarehouse.sketchup.com/",children:"https://3dwarehouse.sketchup.com/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"License:"})," Varies (check individual models)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Case:"})," Simulation environments"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Thingiverse"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," 3D printable model repository"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," 2M+ designs (mechanical parts, tools, objects)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," STL, OBJ, SCAD"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://www.thingiverse.com/",children:"https://www.thingiverse.com/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"License:"})," Creative Commons (varies)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Case:"})," Robot parts, grippers, custom tools"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"GrabCAD"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Professional CAD model library"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," 4.5M+ CAD models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," STEP, STL, SOLIDWORKS, Inventor"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," ",(0,r.jsx)(n.a,{href:"https://grabcad.com/library",children:"https://grabcad.com/library"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Quality:"})," Engineering-grade models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use Case:"})," Robot design, gripper design"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"e4-community-resources-and-forums",children:"E.4 Community Resources and Forums"}),"\n",(0,r.jsx)(n.h3,{id:"e41-ros-discourse",children:"E.4.1 ROS Discourse"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"URL:"})," ",(0,r.jsx)(n.a,{href:"https://discourse.ros.org/",children:"https://discourse.ros.org/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Official ROS community forum"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Categories:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"General: ROS 2 discussions"}),"\n",(0,r.jsx)(n.li,{children:"Next Generation ROS: ROS 2 specific"}),"\n",(0,r.jsx)(n.li,{children:"Using ROS: User questions and tutorials"}),"\n",(0,r.jsx)(n.li,{children:"ROS Projects: Project showcases"}),"\n",(0,r.jsx)(n.li,{children:"Jobs: Career opportunities"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Activity:"})," Very active, responses within hours"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Moderation:"})," Official ROS team and community moderators"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Best Practices:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Search before posting (many common questions answered)"}),"\n",(0,r.jsx)(n.li,{children:"Provide system info (ROS version, OS, hardware)"}),"\n",(0,r.jsx)(n.li,{children:"Include error messages and logs"}),"\n",(0,r.jsx)(n.li,{children:"Tag questions appropriately"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"e42-nvidia-isaac-forums",children:"E.4.2 NVIDIA Isaac Forums"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"URL:"})," ",(0,r.jsx)(n.a,{href:"https://forums.developer.nvidia.com/c/agx-autonomous-machines/isaac/",children:"https://forums.developer.nvidia.com/c/agx-autonomous-machines/isaac/"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Official NVIDIA Isaac support forum"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Subcategories:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Isaac Sim"}),"\n",(0,r.jsx)(n.li,{children:"Isaac ROS"}),"\n",(0,r.jsx)(n.li,{children:"Isaac SDK (legacy)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Support:"})," NVIDIA engineers respond regularly"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," Technical Q&A, bug reports, feature requests"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Related:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Omniverse Forums: ",(0,r.jsx)(n.a,{href:"https://forums.developer.nvidia.com/c/omniverse/",children:"https://forums.developer.nvidia.com/c/omniverse/"})]}),"\n",(0,r.jsxs)(n.li,{children:["Jetson Forums: ",(0,r.jsx)(n.a,{href:"https://forums.developer.nvidia.com/c/agx-autonomous-machines/jetson-embedded-systems/",children:"https://forums.developer.nvidia.com/c/agx-autonomous-machines/jetson-embedded-systems/"})]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"e43-github-repositories",children:"E.4.3 GitHub Repositories"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Awesome Robotics"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"URL:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/kiloreux/awesome-robotics",children:"https://github.com/kiloreux/awesome-robotics"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Curated list of robotics resources"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," Libraries, courses, papers, competitions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Topics:"})," ROS, simulators, vision, planning, control"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Awesome ROS 2"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"URL:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/fkromer/awesome-ros2",children:"https://github.com/fkromer/awesome-ros2"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," ROS 2 specific resources"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," Packages, tutorials, presentations, books"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Updates:"})," Community-maintained, regularly updated"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Awesome Robot Descriptions"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"URL:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/robot-descriptions/awesome-robot-descriptions",children:"https://github.com/robot-descriptions/awesome-robot-descriptions"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Collection of robot URDF/MJCF models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," 200+ robot descriptions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Format:"})," URDF, MJCF (MuJoCo)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Open Robotics GitHub"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"URL:"})," ",(0,r.jsx)(n.a,{href:"https://github.com/osrf",children:"https://github.com/osrf"})," (Open Robotics), ",(0,r.jsx)(n.a,{href:"https://github.com/ros2",children:"https://github.com/ros2"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," Official ROS/Gazebo repositories"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Examples:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"ros2/ros2: ROS 2 meta-repository"}),"\n",(0,r.jsx)(n.li,{children:"gazebosim: Gazebo simulation"}),"\n",(0,r.jsx)(n.li,{children:"osrf/urdf_tutorial: URDF learning resources"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"e44-discord-communities",children:"E.4.4 Discord Communities"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"ROS Discord"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Invite:"})," ",(0,r.jsx)(n.a,{href:"https://discord.gg/ros",children:"https://discord.gg/ros"})," (check ROS Discourse for current link)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Members:"})," 5,000+"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Channels:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"#ros2-help: Technical support"}),"\n",(0,r.jsx)(n.li,{children:"#showcase: Project demonstrations"}),"\n",(0,r.jsx)(n.li,{children:"#nav2: Navigation stack"}),"\n",(0,r.jsx)(n.li,{children:"#moveit: Motion planning"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Activity:"})," Very active, real-time help"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Robotics & AI Discord"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Description:"})," Community for robotics enthusiasts"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Topics:"})," Hobbyist and professional robotics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Channels:"})," Hardware, software, projects, careers"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Isaac Sim Community Discord"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Access:"})," Through NVIDIA Developer forums"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Content:"})," Isaac Sim users, tips, troubleshooting"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Activity:"})," Growing community"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"e45-research-conferences",children:"E.4.5 Research Conferences"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Major Robotics Conferences:"})}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Conference"}),(0,r.jsx)(n.th,{children:"Acronym"}),(0,r.jsx)(n.th,{children:"Focus"}),(0,r.jsx)(n.th,{children:"Deadline (typical)"}),(0,r.jsx)(n.th,{children:"Event (typical)"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"International Conference on Robotics and Automation"}),(0,r.jsx)(n.td,{children:"ICRA"}),(0,r.jsx)(n.td,{children:"Broad robotics"}),(0,r.jsx)(n.td,{children:"October"}),(0,r.jsx)(n.td,{children:"May-June"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"IEEE/RSJ International Conference on Intelligent Robots and Systems"}),(0,r.jsx)(n.td,{children:"IROS"}),(0,r.jsx)(n.td,{children:"Intelligent systems"}),(0,r.jsx)(n.td,{children:"March"}),(0,r.jsx)(n.td,{children:"September-October"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Robotics: Science and Systems"}),(0,r.jsx)(n.td,{children:"RSS"}),(0,r.jsx)(n.td,{children:"Robotics theory"}),(0,r.jsx)(n.td,{children:"January"}),(0,r.jsx)(n.td,{children:"July"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Conference on Robot Learning"}),(0,r.jsx)(n.td,{children:"CoRL"}),(0,r.jsx)(n.td,{children:"Learning for robots"}),(0,r.jsx)(n.td,{children:"June"}),(0,r.jsx)(n.td,{children:"November"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Humanoids"}),(0,r.jsx)(n.td,{children:"Humanoids"}),(0,r.jsx)(n.td,{children:"Humanoid robotics"}),(0,r.jsx)(n.td,{children:"June"}),(0,r.jsx)(n.td,{children:"November"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"International Conference on Computer Vision"}),(0,r.jsx)(n.td,{children:"ICCV"}),(0,r.jsx)(n.td,{children:"Vision (biennial)"}),(0,r.jsx)(n.td,{children:"March"}),(0,r.jsx)(n.td,{children:"October"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"Computer Vision and Pattern Recognition"}),(0,r.jsx)(n.td,{children:"CVPR"}),(0,r.jsx)(n.td,{children:"Vision"}),(0,r.jsx)(n.td,{children:"November"}),(0,r.jsx)(n.td,{children:"June"})]})]})]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Conference Resources:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Paper archives:"})," IEEE Xplore, arXiv.org"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Video presentations:"})," YouTube channels (e.g., ICRA, RSS)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Workshop papers:"})," Often on conference websites"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Following Conferences:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Subscribe to mailing lists for CFPs (Call for Papers)"}),"\n",(0,r.jsx)(n.li,{children:"Follow on Twitter/X for announcements"}),"\n",(0,r.jsx)(n.li,{children:"Attend virtually (many offer online participation)"}),"\n",(0,r.jsx)(n.li,{children:"Review open-access papers on arXiv"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"e5-recommended-reading-and-papers",children:"E.5 Recommended Reading and Papers"}),"\n",(0,r.jsx)(n.h3,{id:"e51-classic-robotics-papers",children:"E.5.1 Classic Robotics Papers"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Kinematics and Control:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"A Mathematical Introduction to Robotic Manipulation"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Murray, Li, Sastry"}),"\n",(0,r.jsx)(n.li,{children:"Year: 1994"}),"\n",(0,r.jsx)(n.li,{children:"Topics: Kinematics, dynamics, control"}),"\n",(0,r.jsxs)(n.li,{children:["Access: ",(0,r.jsx)(n.a,{href:"http://www.cds.caltech.edu/~murray/mlswiki/",children:"http://www.cds.caltech.edu/~murray/mlswiki/"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Robot Dynamics and Control"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Spong, Hutchinson, Vidyasagar"}),"\n",(0,r.jsx)(n.li,{children:"Year: 1989 (2nd ed. 2006)"}),"\n",(0,r.jsx)(n.li,{children:"Topics: Dynamics, trajectory planning, control"}),"\n",(0,r.jsx)(n.li,{children:"Classic textbook reference"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Probabilistic Robotics"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Thrun, Burgard, Fox"}),"\n",(0,r.jsx)(n.li,{children:"Year: 2005"}),"\n",(0,r.jsx)(n.li,{children:"Topics: Localization, SLAM, Kalman/particle filters"}),"\n",(0,r.jsx)(n.li,{children:"Essential for mobile robotics"}),"\n",(0,r.jsxs)(n.li,{children:["Free online: ",(0,r.jsx)(n.a,{href:"https://docs.ufpr.br/~danielsantos/ProbabilisticRobotics.pdf",children:"https://docs.ufpr.br/~danielsantos/ProbabilisticRobotics.pdf"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Motion Planning:"})}),"\n",(0,r.jsxs)(n.ol,{start:"4",children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"A Randomized Approach to Robot Path Planning"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Kavraki et al."}),"\n",(0,r.jsx)(n.li,{children:"Year: 1996"}),"\n",(0,r.jsx)(n.li,{children:"Topic: Probabilistic Roadmaps (PRM)"}),"\n",(0,r.jsx)(n.li,{children:"Citation: Foundation of sampling-based planning"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Randomized Kinodynamic Planning"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: LaValle, Kuffner"}),"\n",(0,r.jsx)(n.li,{children:"Year: 2001"}),"\n",(0,r.jsx)(n.li,{children:"Topic: Rapidly-exploring Random Trees (RRT)"}),"\n",(0,r.jsx)(n.li,{children:"Impact: Enabled planning for high-DOF robots"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"SLAM:"})}),"\n",(0,r.jsxs)(n.ol,{start:"6",children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Real-Time Appearance-Based Mapping"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Se, Lowe, Little"}),"\n",(0,r.jsx)(n.li,{children:"Year: 2002"}),"\n",(0,r.jsx)(n.li,{children:"Topic: Visual SLAM with SIFT features"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"ORB-SLAM: A Versatile and Accurate Monocular SLAM System"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Mur-Artal, Montiel, Tardos"}),"\n",(0,r.jsx)(n.li,{children:"Year: 2015"}),"\n",(0,r.jsx)(n.li,{children:"Impact: Widely-used visual SLAM"}),"\n",(0,r.jsxs)(n.li,{children:["Code: ",(0,r.jsx)(n.a,{href:"https://github.com/raulmur/ORB_SLAM2",children:"https://github.com/raulmur/ORB_SLAM2"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"e52-recent-physical-ai-papers",children:"E.5.2 Recent Physical AI Papers"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Simulation and Sim-to-Real:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Sim-to-Real Transfer of Robotic Control with Dynamics Randomization"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Peng et al."}),"\n",(0,r.jsx)(n.li,{children:"Year: 2018, ICRA"}),"\n",(0,r.jsx)(n.li,{children:"Topic: Domain randomization for transfer"}),"\n",(0,r.jsxs)(n.li,{children:["arXiv: ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1710.06537",children:"https://arxiv.org/abs/1710.06537"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Learning Dexterous In-Hand Manipulation"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: OpenAI et al."}),"\n",(0,r.jsx)(n.li,{children:"Year: 2019, IJRR"}),"\n",(0,r.jsx)(n.li,{children:"Topic: Dexterous manipulation with domain randomization"}),"\n",(0,r.jsxs)(n.li,{children:["arXiv: ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1808.00177",children:"https://arxiv.org/abs/1808.00177"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Isaac Gym: High Performance GPU-Based Physics Simulation"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Makoviychuk et al."}),"\n",(0,r.jsx)(n.li,{children:"Year: 2021, NeurIPS"}),"\n",(0,r.jsx)(n.li,{children:"Topic: Massively parallel RL training"}),"\n",(0,r.jsxs)(n.li,{children:["arXiv: ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2108.10470",children:"https://arxiv.org/abs/2108.10470"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Learning-Based Manipulation:"})}),"\n",(0,r.jsxs)(n.ol,{start:"4",children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Zhang et al."}),"\n",(0,r.jsx)(n.li,{children:"Year: 2018, ICRA"}),"\n",(0,r.jsx)(n.li,{children:"Topic: VR teleoperation for data collection"}),"\n",(0,r.jsxs)(n.li,{children:["arXiv: ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1710.04615",children:"https://arxiv.org/abs/1710.04615"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Learning Synergies between Pushing and Grasping with Self-Supervised Deep Reinforcement Learning"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Zeng et al."}),"\n",(0,r.jsx)(n.li,{children:"Year: 2018, IROS"}),"\n",(0,r.jsx)(n.li,{children:"Topic: Push-grasp learning"}),"\n",(0,r.jsxs)(n.li,{children:["arXiv: ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1803.09956",children:"https://arxiv.org/abs/1803.09956"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Transporter Networks: Rearranging the Visual World for Robotic Manipulation"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Zeng et al."}),"\n",(0,r.jsx)(n.li,{children:"Year: 2021, CoRL"}),"\n",(0,r.jsx)(n.li,{children:"Topic: Spatial action representations"}),"\n",(0,r.jsxs)(n.li,{children:["arXiv: ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2010.14406",children:"https://arxiv.org/abs/2010.14406"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Humanoid Locomotion:"})}),"\n",(0,r.jsxs)(n.ol,{start:"7",children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Learning Bipedal Walking On Planned Footsteps For Humanoid Robots"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Peng et al."}),"\n",(0,r.jsx)(n.li,{children:"Year: 2020, CoRL"}),"\n",(0,r.jsx)(n.li,{children:"Topic: Deep RL for humanoid walking"}),"\n",(0,r.jsxs)(n.li,{children:["arXiv: ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2011.10928",children:"https://arxiv.org/abs/2011.10928"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Learning Locomotion Skills Using DeepMimic"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Peng et al."}),"\n",(0,r.jsx)(n.li,{children:"Year: 2018, SIGGRAPH"}),"\n",(0,r.jsx)(n.li,{children:"Topic: Motion imitation for locomotion"}),"\n",(0,r.jsxs)(n.li,{children:["arXiv: ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/1804.02717",children:"https://arxiv.org/abs/1804.02717"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Whole-Body Humanoid Robot Locomotion with Human Reference"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Radosavovic et al."}),"\n",(0,r.jsx)(n.li,{children:"Year: 2024, arXiv"}),"\n",(0,r.jsx)(n.li,{children:"Topic: Human motion retargeting to humanoid"}),"\n",(0,r.jsxs)(n.li,{children:["arXiv: ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2402.04436",children:"https://arxiv.org/abs/2402.04436"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Vision-Language-Action Models:"})}),"\n",(0,r.jsxs)(n.ol,{start:"10",children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"RT-1: Robotics Transformer for Real-World Control at Scale"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Brohan et al. (Google)"}),"\n",(0,r.jsx)(n.li,{children:"Year: 2022, RSS"}),"\n",(0,r.jsx)(n.li,{children:"Topic: Transformer for robot control"}),"\n",(0,r.jsxs)(n.li,{children:["arXiv: ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2212.06817",children:"https://arxiv.org/abs/2212.06817"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Brohan et al. (Google DeepMind)"}),"\n",(0,r.jsx)(n.li,{children:"Year: 2023, CoRL"}),"\n",(0,r.jsx)(n.li,{children:"Topic: VLA models for robotics"}),"\n",(0,r.jsxs)(n.li,{children:["arXiv: ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2307.15818",children:"https://arxiv.org/abs/2307.15818"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Open X-Embodiment: Robotic Learning Datasets and RT-X Models"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Open X-Embodiment Collaboration"}),"\n",(0,r.jsx)(n.li,{children:"Year: 2023, arXiv"}),"\n",(0,r.jsx)(n.li,{children:"Topic: Large-scale multi-robot dataset"}),"\n",(0,r.jsxs)(n.li,{children:["arXiv: ",(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2310.08864",children:"https://arxiv.org/abs/2310.08864"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"e53-textbooks-and-tutorials",children:"E.5.3 Textbooks and Tutorials"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Foundational Textbooks:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Introduction to Robotics: Mechanics and Control" (4th Edition)'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Author: John J. Craig"}),"\n",(0,r.jsx)(n.li,{children:"Publisher: Pearson, 2017"}),"\n",(0,r.jsx)(n.li,{children:"Topics: Kinematics, dynamics, trajectory planning, control"}),"\n",(0,r.jsx)(n.li,{children:"Level: Undergraduate"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Robotics, Vision and Control: Fundamental Algorithms in MATLAB" (3rd Edition)'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Author: Peter Corke"}),"\n",(0,r.jsx)(n.li,{children:"Publisher: Springer, 2023"}),"\n",(0,r.jsx)(n.li,{children:"Topics: Complete robotics toolkit with MATLAB code"}),"\n",(0,r.jsx)(n.li,{children:"Companion: Robotics Toolbox for MATLAB/Python"}),"\n",(0,r.jsxs)(n.li,{children:["Access: ",(0,r.jsx)(n.a,{href:"https://petercorke.com/rvc/",children:"https://petercorke.com/rvc/"})]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Modern Robotics: Mechanics, Planning, and Control"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Kevin Lynch, Frank Park"}),"\n",(0,r.jsx)(n.li,{children:"Publisher: Cambridge University Press, 2017"}),"\n",(0,r.jsx)(n.li,{children:"Topics: Screw theory, kinematics, dynamics"}),"\n",(0,r.jsxs)(n.li,{children:["Free: ",(0,r.jsx)(n.a,{href:"http://modernrobotics.org",children:"http://modernrobotics.org"})," (videos and book)"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Planning Algorithms"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Author: Steven LaValle"}),"\n",(0,r.jsx)(n.li,{children:"Publisher: Cambridge University Press, 2006"}),"\n",(0,r.jsx)(n.li,{children:"Topics: Comprehensive motion planning"}),"\n",(0,r.jsxs)(n.li,{children:["Free: ",(0,r.jsx)(n.a,{href:"http://planning.cs.uiuc.edu/",children:"http://planning.cs.uiuc.edu/"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"ROS and Practical Guides:"})}),"\n",(0,r.jsxs)(n.ol,{start:"5",children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Programming Robots with ROS: A Practical Introduction"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Quigley, Gerkey, Smart"}),"\n",(0,r.jsx)(n.li,{children:"Publisher: O'Reilly, 2015"}),"\n",(0,r.jsx)(n.li,{children:"Topics: ROS 1 fundamentals (concepts apply to ROS 2)"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"A Systematic Approach to Learning Robot Programming with ROS 2"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Newbury, Bohren, Robinson"}),"\n",(0,r.jsx)(n.li,{children:"Publisher: CRC Press, 2024"}),"\n",(0,r.jsx)(n.li,{children:"Topics: ROS 2 development from basics to advanced"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"ROS 2 Tutorials" (Official)'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Access: ",(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/humble/Tutorials.html",children:"https://docs.ros.org/en/humble/Tutorials.html"})]}),"\n",(0,r.jsx)(n.li,{children:"Content: Beginner to advanced tutorials"}),"\n",(0,r.jsx)(n.li,{children:"Format: Online, free"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Deep Learning for Robotics:"})}),"\n",(0,r.jsxs)(n.ol,{start:"8",children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Deep Learning for Robot Perception and Cognition"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Authors: Piater et al."}),"\n",(0,r.jsx)(n.li,{children:"Publisher: Academic Press, 2022"}),"\n",(0,r.jsx)(n.li,{children:"Topics: Vision, learning, semantic understanding"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:'"Reinforcement Learning for Robotics"'})," (Online Course)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Platform: Coursera, edX, YouTube"}),"\n",(0,r.jsx)(n.li,{children:"Instructors: Pieter Abbeel (UC Berkeley), Sergey Levine (UC Berkeley)"}),"\n",(0,r.jsx)(n.li,{children:"Topics: Deep RL, policy gradients, sim-to-real"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Hands-On Resources:"})}),"\n",(0,r.jsxs)(n.ol,{start:"10",children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Practical Robotics in C++"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Author: Lloyd Brombach"}),"\n",(0,r.jsx)(n.li,{children:"Year: 2021"}),"\n",(0,r.jsx)(n.li,{children:"Topics: ROS, OpenCV, hardware interfacing"}),"\n",(0,r.jsx)(n.li,{children:"Code: Extensive examples"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:'"Robot Operating System (ROS) for Absolute Beginners"'})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Author: Lentin Joseph"}),"\n",(0,r.jsx)(n.li,{children:"Publisher: Apress, 2022"}),"\n",(0,r.jsx)(n.li,{children:"Topics: ROS fundamentals with projects"}),"\n",(0,r.jsx)(n.li,{children:"Code: GitHub examples"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"e54-online-courses-and-tutorials",children:"E.5.4 Online Courses and Tutorials"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Coursera:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:'"Modern Robotics" Specialization'})," (Northwestern University)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Instructor: Kevin Lynch"}),"\n",(0,r.jsx)(n.li,{children:"Duration: 6 courses"}),"\n",(0,r.jsx)(n.li,{children:"Topics: Kinematics, dynamics, planning, control"}),"\n",(0,r.jsx)(n.li,{children:"Certificate: Available"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:'"Robotics" Specialization'})," (University of Pennsylvania)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Duration: 5 courses"}),"\n",(0,r.jsx)(n.li,{children:"Topics: Aerial, autonomous, perception, estimation, mobility"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"edX:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:'"Robotics MicroMasters" (University of Pennsylvania)'}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Duration: 4 courses"}),"\n",(0,r.jsx)(n.li,{children:"Topics: Kinematics, mobility, perception, estimation, learning"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"YouTube Channels:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"MATLAB:"})," Robotics tutorials and examples"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Articulated Robotics:"})," Practical ROS 2 tutorials"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"The Construct:"})," ROS learning platform"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Jeremy Morgan:"})," ROS 2 tutorials"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Hands-On Platforms:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"The Construct Sim"})," (",(0,r.jsx)(n.a,{href:"https://www.theconstructsim.com/",children:"https://www.theconstructsim.com/"}),")"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Online ROS development environment"}),"\n",(0,r.jsx)(n.li,{children:"Curated courses and projects"}),"\n",(0,r.jsx)(n.li,{children:"Simulation included"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Robot Ignite Academy"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Structured ROS 2 learning paths"}),"\n",(0,r.jsx)(n.li,{children:"Simulation-based exercises"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"This appendix provided comprehensive resource listings for Physical AI development:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Datasets"}),": Manipulation (YCB, ACRONYM), Navigation (TUM, EuRoC, KITTI), Human Motion (CMU, Human3.6M, AMASS), Grasping (Dex-Net, ContactDB)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Pre-trained Models"}),": Object detection (YOLO, Detectron2), segmentation (SAM, Mask R-CNN), pose estimation (OpenPose, HRNet), speech (Whisper, Vosk)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"3D Assets"}),": Robot URDFs (ROS-Industrial, Clearpath, Unitree), environments (Gazebo, AWS, NVIDIA), object meshes (ShapeNet, ModelNet)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Community"}),": Forums (ROS Discourse, NVIDIA), GitHub repositories, Discord servers, research conferences"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reading"}),": Classic papers (Probabilistic Robotics, SLAM), recent work (sim-to-real, VLA models), textbooks (Craig, Lynch, Corke), online courses"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"These resources enable effective research, development, and continuous learning in robotics. Bookmark key repositories and join active communities to stay current with rapidly evolving Physical AI technologies."}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Recommended Starting Points:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"New to ROS 2: Official tutorials + Articulated Robotics YouTube"}),"\n",(0,r.jsx)(n.li,{children:"Need datasets: Start with COCO (vision), YCB (manipulation), TUM (SLAM)"}),"\n",(0,r.jsx)(n.li,{children:"Pre-trained models: PyTorch Hub, Hugging Face, NVIDIA NGC"}),"\n",(0,r.jsx)(n.li,{children:"Community help: ROS Discourse (async), ROS Discord (real-time)"}),"\n",(0,r.jsx)(n.li,{children:"Deep dive: Modern Robotics textbook + online course"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Stay engaged with conferences (ICRA, IROS, RSS) and follow key researchers on arXiv for latest developments."})]})}function a(s={}){const{wrapper:n}={...(0,l.R)(),...s.components};return n?(0,r.jsx)(n,{...s,children:(0,r.jsx)(h,{...s})}):h(s)}},8453(s,n,e){e.d(n,{R:()=>t,x:()=>o});var i=e(6540);const r={},l=i.createContext(r);function t(s){const n=i.useContext(l);return i.useMemo(function(){return"function"==typeof s?s(n):{...n,...s}},[n,s])}function o(s){let n;return n=s.disableParentContext?"function"==typeof s.components?s.components(r):s.components||r:t(s.components),i.createElement(l.Provider,{value:n},s.children)}}}]);